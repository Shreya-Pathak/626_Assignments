{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import nltk\nfrom nltk.stem.porter import *\nfrom nltk.classify import MaxentClassifier\nimport tensorflow as tf\nimport numpy as np\nfrom io import open\nimport os\nprint(os.getcwd())","execution_count":1,"outputs":[{"output_type":"stream","text":"/kaggle/working\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":2,"outputs":[{"output_type":"stream","text":"REPLICAS:  1\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def loadGloveModel(File):\n    print(\"Loading Glove Model\")\n    f = open(File,'r')\n    gloveModel = {}\n    for line in f:\n        splitLines = line.split()\n        word = splitLines[0]\n        wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n        gloveModel[word] = wordEmbedding\n    print(len(gloveModel),\" words loaded!\")\n    return gloveModel\nglovemod=loadGloveModel(\"/kaggle/input/glove6b50dtxt/glove.6B.50d.txt\")","execution_count":3,"outputs":[{"output_type":"stream","text":"Loading Glove Model\n400000  words loaded!\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"boi_full_list = [] #store all the boi tags that occur in the training set\nBO_list = ['B', 'I','O']\nlabeled_features = []\nwordStartList = []\nboi_end_list = []","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"POS_dict={'X':0}\nposcnt=1\nwith open(\"/kaggle/input/conll-corpora/conll2000/conll2000/train.txt\", \"r\") as f:\n    for line in f:\n        line=line.strip()\n        if line!='':\n            POS_tag=line.split()[1]\n            if POS_tag not in POS_dict:\n                POS_dict[POS_tag]=poscnt\n                poscnt+=1\ndef get_vector_POS(tag):\n    ans=[0]*len(POS_dict)\n    ans[POS_dict[tag]]=1\n    return ans","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(POS_dict)\n# print(\"hey\")\n# output_file = open(\"bo_output.txt\", \"wb\")","execution_count":6,"outputs":[{"output_type":"stream","text":"{'X': 0, 'NN': 1, 'IN': 2, 'DT': 3, 'VBZ': 4, 'RB': 5, 'VBN': 6, 'TO': 7, 'VB': 8, 'JJ': 9, 'NNS': 10, 'NNP': 11, ',': 12, 'CC': 13, 'POS': 14, '.': 15, 'VBP': 16, 'VBG': 17, 'PRP$': 18, 'CD': 19, '``': 20, \"''\": 21, 'VBD': 22, 'EX': 23, 'MD': 24, '#': 25, '(': 26, '$': 27, ')': 28, 'NNPS': 29, 'PRP': 30, 'JJS': 31, 'WP': 32, 'RBR': 33, 'JJR': 34, 'WDT': 35, 'WRB': 36, 'RBS': 37, 'PDT': 38, 'RP': 39, ':': 40, 'FW': 41, 'WP$': 42, 'SYM': 43, 'UH': 44}\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"boi_full_list = [] #store all the boi tags that occur in the training set\nlabeled_features = []\nwordStartList = []\nboi_end_list = []\ntraining_file = open(\"/kaggle/input/conll-corpora/conll2000/conll2000/train.txt\", \"r\")\nprevious_BOI = \"start\"\ninput_file = training_file\nchange_of_sentence_flag=1\nst = False\ni = 0\nfor line in input_file:\n    #print(line)\n    s = re.match(r'^\\s*$', line)  #find empty line\n    if s:\n        i=0\n        change_of_sentence_flag = 1\n        previous_BOI = \"start\"\n    else: \n        sentenceList = line.split()\n        word = sentenceList[0]\n        tag = sentenceList[1]\n        boi = (sentenceList[2].split('-'))[0]\n        #print(boi)\n        \n        #store words that are begining of the sentence\n        if change_of_sentence_flag == 1:\n            wordStartList.append(word)\n            if st:\n                boi_end_list.append(boi_full_list[-1])\n            change_of_sentence_flag = 0\n        boi_full_list.append(boi)\n        prev_w, prev_t, prev_w2, prev_t2, prev_bi, prev_bi2 = 'X', 'X', 'X', 'X', 'start', 'start'\n        next_w, next_t, next_w2, next_t2 = 'X', 'X', 'X', 'X'\n        if i>0:\n            prev_w = labeled_features[-1][0]\n            prev_t = labeled_features[-1][1]\n            prev_bi = labeled_features[-1][2]\n            labeled_features[-1][9] = word\n            labeled_features[-1][10] = tag\n        if i>1:\n            prev_w2 = labeled_features[-2][0]\n            prev_t2 = labeled_features[-2][1]\n            prev_bi2 = labeled_features[-2][2]\n            labeled_features[-2][11] = word\n            labeled_features[-2][12] = tag\n        item = [word, tag, boi, prev_w, prev_t, prev_bi, prev_w2, prev_t2,prev_bi2, next_w, next_t, next_w2, next_t2 ]\n        #print(item) if s:\n        w=0\n        change_of_sentence_flag = 1\n        previous_BOI = \"start\"\n#         test_data.append(small_list)\n#         test_labels.append(boi_list)\n        labeled_features.append(item)\n        previous_BOI = boi\n        st = True\n        i+=1\n        #labeled_features \n\nboi_end_list.append(boi_full_list[-1])\n# print(\"labeled_features\")\n# print(labeled_features)\n#unlabeled_features = []\n\ninput_file.close()","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[print(x) for x in labeled_features[35:40]]","execution_count":8,"outputs":[{"output_type":"stream","text":"['deficits', 'NNS', 'I', 'near-record', 'JJ', 'I', \"'s\", 'POS', 'B', '.', '.', 'X', 'X']\n['.', '.', 'O', 'deficits', 'NNS', 'I', 'near-record', 'JJ', 'I', 'X', 'X', 'X', 'X']\n['Chancellor', 'NNP', 'O', 'X', 'X', 'start', 'X', 'X', 'start', 'of', 'IN', 'the', 'DT']\n['of', 'IN', 'B', 'Chancellor', 'NNP', 'O', 'X', 'X', 'start', 'the', 'DT', 'Exchequer', 'NNP']\n['the', 'DT', 'B', 'of', 'IN', 'B', 'Chancellor', 'NNP', 'O', 'Exchequer', 'NNP', 'Nigel', 'NNP']\n","name":"stdout"},{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"[None, None, None, None, None]"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"stemmer = PorterStemmer()\nprint(len(labeled_features))","execution_count":9,"outputs":[{"output_type":"stream","text":"211727\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def MEMM_features(item):\n    # 0word, 1tag, 2boi, 3prev_w, 4prev_t, 5prev_bi, 6prev_w2, 7prev_t2,8prev_bi2, 9next_w, 10next_t, 11next_w2, 12next_t2  \n    features = {}\n    allbois=['B', 'I','O','start']\n    emb1 = glovemod.get(item[0].lower(),glovemod['.'])\n    for j in range(emb1.shape[0]):\n            features[\"cw\"+str(j)]=emb1[j]\n            \n    emb1 = glovemod.get(item[3].lower(),glovemod['.'])\n    for j in range(emb1.shape[0]):\n            features[\"pw\"+str(j)]=emb1[j]\n    \n    emb1 = glovemod.get(item[6].lower(),glovemod['.'])\n    for j in range(emb1.shape[0]):\n            features[\"pw2\"+str(j)]=emb1[j]\n            \n    emb1 = glovemod.get(item[9].lower(),glovemod['.'])\n    for j in range(emb1.shape[0]):\n            features[\"nw\"+str(j)]=emb1[j]\n            \n    emb1 = glovemod.get(item[11].lower(),glovemod['.'])\n    for j in range(emb1.shape[0]):\n            features[\"nw2\"+str(j)]=emb1[j]\n\n    cnter=0\n    for x in get_vector_POS(item[1]):\n        features['current_tag'+str(cnter)]=x\n        cnter+=1\n    \n    cnter=0\n    for x in get_vector_POS(item[4]):\n        features['prev_tag'+str(cnter)]=x\n        cnter+=1\n    cnter=0\n    for x in get_vector_POS(item[7]):\n        features['[prev_tag2'+str(cnter)]=x\n        cnter+=1\n    cnter=0\n    for x in get_vector_POS(item[10]):\n        features['next_tag'+str(cnter)]=x\n        cnter+=1\n    cnter=0\n    for x in get_vector_POS(item[12]):\n        features['next_tag2'+str(cnter)]=x\n        cnter+=1   \n#     features['current_tag'] = item[1]\n#     features['prev_tag'] = item[4]\n#     features['prev_tag2'] = item[7]\n#     features['next_tag'] = item[10]\n#     features['next_tag2'] = item[12]\n    for i in range(4):\n        features['prev_bi'+str(i)]=0\n    features['prev_bi'+str(allbois.index(item[5]))]=1\n#     features['prev_bi2'] = item[8]\n#     puc = '-'.decode(\"utf-8\")\n#      #some char is outof ASCII\n#     print (word)\n    features['capitalization'] = int(word[0].isupper())\n    features['start_of_sentence'] = int(word in wordStartList)\n    features['cap_start'] = int(word not in wordStartList and word[0].isupper())\n    #features['suffix'] =  item[0].replace(stemmer.stem(item[0]),\"\")\n    #features['previous_NC'] = previous_BOI\n    token = item[0]\n    features['ends_with_ly']    = float(token.endswith('ly'))\n    features['ends_with_ment']  = float(token.endswith('ment'))\n    features['ends_with_able']  = float(token.endswith('able') or token.endswith('ible'))\n    features['ends_with_fy' ]   = float(token.endswith('fy'))\n    features['ends_with_al']    = float(token.endswith('al'))\n#     print(token)\n#     print(features['ends_with_ly'])\n    f1=list(features.items())\n    features=[x[1] for x in f1]\n    return features","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"r=np.array(MEMM_features(labeled_features[0]))\nprint(r)\nprint(r.dtype)","execution_count":11,"outputs":[{"output_type":"stream","text":"['-0.4772' '-0.17997' '0.5497' '-0.33583' '0.92515' '-0.32671' '0.19904'\n '0.37164' '-0.29774' '0.55386' '0.38082' '-0.086124' '-0.81728' '-0.1773'\n '-0.39483' '0.68326' '-0.15394' '-0.75678' '0.23731' '-0.57452'\n '0.034714' '0.73614' '-0.20833' '-1.3799' '0.75239' '-1.2134' '-0.26831'\n '0.19753' '0.20667' '0.98848' '2.9728' '1.592' '0.32605' '-0.5557'\n '-0.44579' '-0.65134' '-0.45836' '-0.029303' '0.36805' '-1.3424'\n '-0.56094' '-0.51221' '-0.37997' '-0.31218' '-0.044253' '0.39778'\n '-0.22441' '0.9186' '0.81454' '-0.21249' '0.52669' '1.6009' '1.822'\n '0.95336' '0.5721' '0.383' '0.76729' '-1.6539' '0.23842' '-0.31595'\n '0.012596' '0.1561' '-0.67865' '0.18208' '0.54657' '-0.15427' '-0.93907'\n '-0.044995' '-1.0153' '-0.16645' '0.52669' '1.6009' '1.822' '0.95336'\n '0.5721' '0.383' '0.76729' '-1.6539' '0.23842' '-0.31595' '2.2896'\n '-0.15124' '-0.68966' '-0.082019' '0.38181' '-0.34059' '1.1077'\n '-0.39044' '-0.091761' '0.11585' '0.53802' '-0.29055' '0.73364' '0.22701'\n '-0.6085' '1.1221' '0.52212' '-0.83873' '0.043014' '0.6167' '0.012596'\n '0.1561' '-0.67865' '0.18208' '0.54657' '-0.15427' '-0.93907' '-0.044995'\n '-1.0153' '-0.16645' '-1.1026' '-0.048263' '1.0927' '0.14736' '-0.37286'\n '0.0090402' '-0.80239' '-0.4917' '0.30821' '-0.087714' '2.2896'\n '-0.15124' '-0.68966' '-0.082019' '0.38181' '-0.34059' '1.1077'\n '-0.39044' '-0.091761' '0.11585' '0.53802' '-0.29055' '0.73364' '0.22701'\n '-0.6085' '1.1221' '0.52212' '-0.83873' '0.043014' '0.6167' '0.33042'\n '0.24995' '-0.60874' '0.10923' '0.036372' '0.151' '-0.55083' '-0.074239'\n '-0.092307' '-0.32821' '0.09598' '-0.82269' '-0.36717' '-0.67009'\n '0.42909' '0.016496' '-0.23573' '0.12864' '-1.0953' '0.43334' '0.418'\n '0.24968' '-0.41242' '0.1217' '0.34527' '-0.044457' '-0.49688' '-0.17862'\n '-0.00066023' '-0.6566' '3.8631' '-0.17786' '-0.082434' '-0.62698'\n '0.26497' '-0.057185' '-0.073521' '0.46103' '0.30862' '0.12498'\n '-0.48609' '-0.0080272' '0.031184' '-0.36576' '-0.42699' '0.42164'\n '-0.11666' '-0.50703' '-0.027273' '-0.53285' '0.27843' '-0.14767'\n '-0.55677' '0.14658' '-0.0095095' '0.011658' '0.10204' '-0.12792'\n '-0.8443' '-0.12181' '-0.016801' '-0.33279' '-0.1552' '-0.23131'\n '-0.19181' '-1.8823' '-0.76746' '0.099051' '-0.42125' '-0.19526' '4.0071'\n '-0.18594' '-0.52287' '-0.31681' '0.00059213' '0.0074449' '0.17778'\n '-0.15897' '0.012041' '-0.054223' '-0.29871' '-0.15749' '-0.34758'\n '-0.045637' '-0.44251' '0.18785' '0.0027849' '-0.18411' '-0.11514'\n '-0.78581' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' 'X' '0' '0' '0'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' 'DT' '0' '0' '0'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n '0' '0' 'NN' 'X' 'IN' '0' '0' '0' '1' '0' '1' '0' '0.0' '0.0' '0.0' '0.0'\n '0.0']\n<U32\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"labeled_featuresets = [MEMM_features(item) for item in labeled_features]\n# r=np.array(labeled_featuresets)\n# print(np.shape(r))\n# print(r.dtype)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(labeled_featuresets[0][-10:])","execution_count":13,"outputs":[{"output_type":"stream","text":"[0, 1, 0, 1, 0, 0.0, 0.0, 0.0, 0.0, 0.0]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_set = np.array(labeled_featuresets)\nprint(len(train_set))\nprint(train_set[0])\nprint(train_set.dtype)","execution_count":14,"outputs":[{"output_type":"stream","text":"211727\n['-0.4772' '-0.17997' '0.5497' '-0.33583' '0.92515' '-0.32671' '0.19904'\n '0.37164' '-0.29774' '0.55386' '0.38082' '-0.086124' '-0.81728' '-0.1773'\n '-0.39483' '0.68326' '-0.15394' '-0.75678' '0.23731' '-0.57452'\n '0.034714' '0.73614' '-0.20833' '-1.3799' '0.75239' '-1.2134' '-0.26831'\n '0.19753' '0.20667' '0.98848' '2.9728' '1.592' '0.32605' '-0.5557'\n '-0.44579' '-0.65134' '-0.45836' '-0.029303' '0.36805' '-1.3424'\n '-0.56094' '-0.51221' '-0.37997' '-0.31218' '-0.044253' '0.39778'\n '-0.22441' '0.9186' '0.81454' '-0.21249' '0.52669' '1.6009' '1.822'\n '0.95336' '0.5721' '0.383' '0.76729' '-1.6539' '0.23842' '-0.31595'\n '0.012596' '0.1561' '-0.67865' '0.18208' '0.54657' '-0.15427' '-0.93907'\n '-0.044995' '-1.0153' '-0.16645' '0.52669' '1.6009' '1.822' '0.95336'\n '0.5721' '0.383' '0.76729' '-1.6539' '0.23842' '-0.31595' '2.2896'\n '-0.15124' '-0.68966' '-0.082019' '0.38181' '-0.34059' '1.1077'\n '-0.39044' '-0.091761' '0.11585' '0.53802' '-0.29055' '0.73364' '0.22701'\n '-0.6085' '1.1221' '0.52212' '-0.83873' '0.043014' '0.6167' '0.012596'\n '0.1561' '-0.67865' '0.18208' '0.54657' '-0.15427' '-0.93907' '-0.044995'\n '-1.0153' '-0.16645' '-1.1026' '-0.048263' '1.0927' '0.14736' '-0.37286'\n '0.0090402' '-0.80239' '-0.4917' '0.30821' '-0.087714' '2.2896'\n '-0.15124' '-0.68966' '-0.082019' '0.38181' '-0.34059' '1.1077'\n '-0.39044' '-0.091761' '0.11585' '0.53802' '-0.29055' '0.73364' '0.22701'\n '-0.6085' '1.1221' '0.52212' '-0.83873' '0.043014' '0.6167' '0.33042'\n '0.24995' '-0.60874' '0.10923' '0.036372' '0.151' '-0.55083' '-0.074239'\n '-0.092307' '-0.32821' '0.09598' '-0.82269' '-0.36717' '-0.67009'\n '0.42909' '0.016496' '-0.23573' '0.12864' '-1.0953' '0.43334' '0.418'\n '0.24968' '-0.41242' '0.1217' '0.34527' '-0.044457' '-0.49688' '-0.17862'\n '-0.00066023' '-0.6566' '3.8631' '-0.17786' '-0.082434' '-0.62698'\n '0.26497' '-0.057185' '-0.073521' '0.46103' '0.30862' '0.12498'\n '-0.48609' '-0.0080272' '0.031184' '-0.36576' '-0.42699' '0.42164'\n '-0.11666' '-0.50703' '-0.027273' '-0.53285' '0.27843' '-0.14767'\n '-0.55677' '0.14658' '-0.0095095' '0.011658' '0.10204' '-0.12792'\n '-0.8443' '-0.12181' '-0.016801' '-0.33279' '-0.1552' '-0.23131'\n '-0.19181' '-1.8823' '-0.76746' '0.099051' '-0.42125' '-0.19526' '4.0071'\n '-0.18594' '-0.52287' '-0.31681' '0.00059213' '0.0074449' '0.17778'\n '-0.15897' '0.012041' '-0.054223' '-0.29871' '-0.15749' '-0.34758'\n '-0.045637' '-0.44251' '0.18785' '0.0027849' '-0.18411' '-0.11514'\n '-0.78581' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1' '0' 'X' '0' '0' '0'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n '0' '0' '0' '1' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' 'DT' '0' '0' '0'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '1'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n '0' '0' 'NN' 'X' 'IN' '0' '0' '0' '1' '0' '1' '0' '0.0' '0.0' '0.0' '0.0'\n '0.0']\n<U32\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_shape = train_set[0].shape[0]\nBO_list = ['B', 'I','O']\ntrain_labels=np.array([BO_list.index(x) for x in boi_full_list])","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print((train_labels.dtype))","execution_count":16,"outputs":[{"output_type":"stream","text":"int64\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(train_set.shape)\nprint(train_set.dtype)\nprint(type(train_set[0]))\nprint(type(train_set))","execution_count":17,"outputs":[{"output_type":"stream","text":"(211727, 460)\n<U32\n<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\nwith strategy.scope():\n    model = tf.keras.models.Sequential()\n    model.add(tf.keras.Input(shape=(input_shape,)))\n    #model.add(tf.keras.layers.Dropout(0.2))\n    model.add(tf.keras.layers.Dense(len(BO_list), activation='softmax'))# changed here\n    model.compile(loss='sparse_categorical_crossentropy',optimizer=tf.keras.optimizers.Adam(0.001),metrics=['accuracy'])\n    model.fit(x=train_set,y=train_labels,validation_split=0.1,epochs=5,batch_size = 128 * strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import joblib\njoblib.dump(train_set, 'train_features.pkl')\njoblib.dump(train_labels,'train_labels.pkl')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dicE = {} #temporarry dic \n# countTag = 0\n# countEnd = 0\n# #calculate the prior (End|state) = C(state, End)/C(state) \n# for i in BO_list:\n#     for j  in range(len(boi_end_list)):\n#         for f in boi_full_list:\n#             if j == 0:\n#                 if i == f:\n#                     countTag = countTag + 1 \n#         if i == boi_end_list[j]:\n#             countEnd = countEnd + 1 \n#     ProbE = format(countEnd/(countTag*1.0), '.5f')\n#     dicE.update({i: {\"END\":ProbE}})\n\n#     countEnd = 0\n#     countTag = 0","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def MEMM(preds):\n    BOI_list = ['B', 'I','O']\n    tRange = len(BOI_list)\n    wRange = int(len(preds)//3+1)\n    #print(wRange)\n#     w1 = wordList[0] #the first word of the sentence\n#     t1 = tagList[0]\n#     wp1 = 'X'\n#     wp2 = 'X'\n#     tp1 = 'X'\n#     tp2 = 'X'\n#     wn1 = 'X' if wRange<2 else wordList[1]\n#     wn2 = 'X' if wRange<3 else wordList[2]\n#     tn1 = 'X' if wRange<2 else tagList[1]\n#     tn2 = 'X' if wRange<3 else tagList[2]\n    \n    #word, tag, boi, prev_w, prev_t, prev_bi, prev_w2, prev_t2, next_w, next_t, next_w2, next_t2\n\n    max_len = 300\n    viterbi = [[0 for x in range(max_len)] for x in range(max_len)] \n    backpointer = [['' for x in range(max_len)] for x in range(max_len)] \n    #intialization\n    #item = [w1, t1, 'BIO', wp1, tp1, 'start', wp2, tp2,'start', wn1, tn1, wn2, tn2 ]\n    #probability = model.predict(np.array([MEMM_features(item)]))\n    for t in range(tRange):#t = 0,1,2\n        posterior = float(preds[0][t])\n        #print (\"boi: \" + BOI_list[t] + ' posterior (start)' + str(posterior))\n        #score transition 0(start) -> q given w1\n        viterbi[t][1] = posterior\n        backpointer[t][1] = 0 #stand for q0 (start point)\n\n    #for word w from 2 to T\n    maxViterbi = 0\n    maxPreviousState = 0 \n    maxPreTerminalProb = 0\n    for w in range (1, wRange):\n#         w1 = wordList[w] #the first word of the sentence\n#         t1 = tagList[w]\n#         wp1 = wordList[w-1]\n#         wp2 = 'X' if w<2 else wordList[w-2]\n#         tp1 = tagList[w-1]\n#         tp2 = 'X' if w<2 else tagList[w-2]\n#         wn1 = 'X' if w>wRange-2 else wordList[w+1]\n#         wn2 = 'X' if w>wRange-3 else wordList[w+2]\n#         tn1 = 'X' if w>wRange-2 else tagList[w+1]\n#         tn2 = 'X' if w>wRange-3 else tagList[w+2]\n#         item = [w1, t1, 'BIO', wp1, tp1, 'BIO', wp2, tp2,'ununsed', wn1, tn1, wn2, tn2 ]\n#         pr = [0 for x in range(tRange)]\n#         for t in range(tRange):\n#             item[5]=BOI_list[t]\n#             pr[t] = model.predict(np.array([MEMM_features(item)])) \n        for t in range (tRange):\n            #find max verterbi = max (previous * posterior)\t\n#             word = wordList[w]\n#             tag = tagList[w]\n            #probability = maxent_classifier.prob_classify(MEMM_features(word,tag,BOI_list[0] )) \n            posterior = float(preds[(w-1)*3+1][t])\n            maxViterbi = float(viterbi[0][w]) * posterior\n            maxPreviousState = 0\n            for i in range (1, tRange):\n#                 word = wordList[w]\n#                 tag = tagList[w]\n#                 probability = maxent_classifier.prob_classify(MEMM_features(word,tag,BOI_list[i] )) \n                posterior = float(preds[(w-1)*3+1+i][t])\n                if float(viterbi[i][w]) * posterior > maxViterbi:\n                    maxViterbi = float(viterbi[i][w]) * posterior\n                    maxPreviousState = i #content BOI_List[i]\n            viterbi[t][w+1] = maxViterbi\n            backpointer[t][w+1] = BOI_list[maxPreviousState] #points to the matrix x axis (max previous)\n\n            maxViterbi = 0\n            maxPreviousState = 0 \n            maxPreTerminalProb = 0\n    #termination step\n    #viterbi[qF, T] = max (viterbi[s,T] *as,qF)\n    maxPreTerminalProb = float(viterbi[0][wRange] )#* float(dicE[BOI_list[0]][\"END\"])\n    \n    maxPreviousState = 0\n    for i in range (1, tRange):\n\n        if float(viterbi[i][wRange]) > maxPreTerminalProb:\n            maxPreTerminalProb = float(viterbi[i][wRange]) #* float(dicE[BOI_list[i]][\"END\"]) \n\n            maxPreviousState = i\n\n#print (\"maxPreTerminalProb: \" + str(maxPreTerminalProb))\n    viterbi[tRange][wRange+1] = maxPreTerminalProb \n    backpointer[tRange][wRange+1] = BOI_list[maxPreviousState]\n#return POS tag path \n    pathReverse = [BOI_list[maxPreviousState]]\n    maxPreviousTag = BOI_list[maxPreviousState]\n\n    i = 0\n    while i < (wRange -1):\n        pathReverse.append(backpointer[BOI_list.index(maxPreviousTag)][wRange - i])\n        maxPreviousTag = backpointer[BOI_list.index(maxPreviousTag)][wRange - i]\n        i = i + 1 \n\n#reverse the path to make it correct\n    index = len(pathReverse)\n    path = []\n    while index >= 1 :\n        path.append(pathReverse[index - 1])\n        index = index -1 \n    return path","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"testing_file = open(\"/kaggle/input/conll-corpora/conll2000/conll2000/test.txt\", \"r\")\nwordList = [] #store words in a sentence\ntagList = [] #store part-of-speech tag in a sentence \nboiList = [] #store boi tags in a sentence \nind_list=[]\nindl_list = []\nboi_list=[]\nallWords = []\n#prob_table = {} #stpre the posterior\nprevious_BOI = \"start\"\nBOI_list = ['B', 'I','O']\ntRange = len(BOI_list)\ntot_corr=0\ntot = 0\n\nw=0\ntest_data = []\ntest_labels = []\ninput_file = testing_file\nfor line in input_file:\n    \n    if line.strip() != '': #if not empty do following \n        sentenceList = line.split()\n        word = sentenceList[0]\n        tag = sentenceList[1]\n        boi = (sentenceList[2].split('-'))[0]\n        wordList.append(word)\n        tagList.append(tag)\n        boiList.append(boi)\n        wp1 = 'X' if w==0 else wordList[w-1]\n        wp2 = 'X' if w<2 else wordList[w-2]\n        tp1 = 'X' if w==0 else tagList[w-1]\n        tp2 = 'X' if w<2 else tagList[w-2]\n        wn1 = 'X' #if w>wRange-2 else wordList[w+1]\n        wn2 = 'X' #if w>wRange-3 else wordList[w+2]\n        tn1 = 'X' #if w>wRange-2 else tagList[w+1]\n        tn2 = 'X' #if w>wRange-3 else tagList[w+2]\n        item = [word, tag, 'BIO', wp1, tp1, 'BIO', wp2, tp2,'ununsed', wn1, tn1, wn2, tn2 ]\n        if w==1:\n            test_data[-1][9] = word\n            test_data[-1][10] = tag\n        elif w>1:\n            for i in range(tRange):\n                test_data[-1-i][9] = word\n                test_data[-1-i][10] = tag\n        if w==2:\n            test_data[-1-tRange][11] = word\n            test_data[-1-tRange][12] = tag\n        elif w>2:\n            for i in range(tRange):\n                test_data[-1-tRange-i][11] = word\n                test_data[-1-tRange-i][12] = tag\n        if w==0:\n            item1 = item.copy()\n            item1[5] = \"start\"\n            test_data.append(item1)\n            #test_labels.append(boi)\n        else:\n            for i in range(tRange):\n                item1 = item.copy()\n                item1[5] = BOI_list[i]\n                #print(BOI_list[i])\n                test_data.append(item1)\n                #test_labels.append(boi)\n        w+=1\n        test_labels.append(boi)\n        #print (word, tag, boi)\n#store words that are begining of the sentence\n#store tags that are the begining of the sentence\n#store the end of sentence tag in tagEndList\n        if change_of_sentence_flag == 1:\n            wordStartList.append(word)\n            change_of_sentence_flag = 0\n    s = re.match(r'^\\s*$', line)  #find empty line\n    if s:\n        w=0\n        change_of_sentence_flag = 1\n        previous_BOI = \"start\"\n        ind_list.append(len(test_data)-1)\n        indl_list.append(len(test_labels)-1)\n#         test_data.append(small_list)\n#         test_labels.append(boi_list)\n        #path = MEMM(wordList, tagList) #list of BOI_tags returned by HMM function call\n#         print(path)\n#         metric = np.array(boiList)==np.array(path)\n#         acc = np.sum(metric)/len(path)\n# #         print(boiList)\n# #         print(path)\n#         print(acc)\n# #         print(metric)\n#         tot_corr += np.sum(metric)\n#         tot += len(path)\n        allWords+=wordList\n        wordList = [] # refresh word list\n        tagList = []\n        boiList = []\n        #small_list = []\n        #boi_list = []\n        #break\n        #prob_table = {}#refresh prob_table\n\ninput_file.close()\n#print(tot_corr/tot)\n#output_file.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_data[1:4])\nprint(test_labels[0])\nprint(len(test_data))\nprint(len(test_labels))\nprint(len(ind_list))\nprint(len(indl_list))\nprint(ind_list[0:10])\nprint(indl_list[0:10])\nprint(len(allWords))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"final_test = [MEMM_features(item) for item in test_data]\ntest_set = np.array(final_test)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(test_set))\nprint(np.sum(test_set[1]==test_set[2]))\nprint(test_set.dtype)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_set.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BO_list = ['B', 'I','O']\ntest_labels_ind=np.array([BO_list.index(x) for x in test_labels])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with strategy.scope():\n    predictions = model.predict(test_set)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prev = 0\nprevl = 0\ntot_corr = 0\ntot = 0\nfinal_pred = []\n#len(ind_list)\nfor i in range(len(ind_list)):\n    x1 = ind_list[i]\n    x2 = indl_list[i]\n    test_list = predictions[prev:x1+1]\n    true_val = test_labels[prevl:x2+1]\n    prev = x1+1\n    prevl = x2+1\n    path = MEMM(test_list)\n    final_pred+=path\n#     print(path)\n#     print(true_val)\n    metric = np.array(true_val)==np.array(path)\n    acc = np.sum(metric)/len(path)\n    #print(acc)\n    tot_corr += np.sum(metric)\n    tot += len(path)\n\nprint(tot_corr/tot)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from matplotlib import pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\ndef get_report(y_true, y_pred, classes):\n    clf_report = classification_report(y_true, y_pred, labels=classes, zero_division=0)\n    clf_report = clf_report.replace('\\n\\n', '\\n')\n    clf_report = clf_report.replace('micro avg', 'micro_avg')\n    clf_report = clf_report.replace('macro avg', 'macro_avg')\n    clf_report = clf_report.replace('weighted avg', 'weighted_avg')\n    clf_report = clf_report.replace(' / ', '/')\n    lines = clf_report.split('\\n')\n\n    class_names, plotMat, support = [], [], []\n    for line in lines[1:]:\n        t = line.strip().split()\n        if len(t) < 2:\n            continue\n        v = [float(x) for x in t[1: len(t) - 1]]\n        if len(v) == 1 : v = v * 3\n        support.append(int(t[-1]))\n        class_names.append(t[0])\n        plotMat.append(v)\n    plotMat = np.array(plotMat)\n    support = np.array(support)\n    return class_names, plotMat, support\n\ndef get_scores(y_true, y_pred, classes):\n    correct, wrong = {}, {}\n    for tag in classes:\n        correct[tag] = 0\n        wrong[tag] = 0\n        \n    for tag, pred in zip(y_true, y_pred):\n        if tag == pred:\n            correct[tag] += 1\n        else:\n            wrong[tag] += 1\n            \n    scores = []\n    total = len(y_true)\n    for tag in classes:\n        cur = np.array([correct[tag], wrong[tag]])\n        scores.append(cur / total)\n    return np.array(scores)\n    \ndef plot_confusion_matrix(classes, mat, normalize=True, cmap=plt.cm.Blues):\n    cm = np.copy(mat)\n    title = 'Confusion Matrix (without normalization)'\n    if normalize:\n        cm = cm.astype('float') / np.sum(cm, axis=1, keepdims=True)\n        title = title.replace('without', 'with')\n    plt.clf()    \n    fig, ax = plt.subplots(figsize=(20,10))\n    ax.set_title(title, y=-0.06, fontsize=22)\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.set_label_position('top')\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.clim(vmin=0.0, vmax=1.0)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = '.2f' if normalize else 'd'\n    thresh = np.max(cm) / 2\n    thresh = 1 / 2\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            color = \"white\" if (cm[i, j] > thresh) else \"black\"\n            plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=color)\n    plt.ylabel('True label',fontsize=22)\n    plt.xlabel('Predicted label', fontsize=22)\n    plt.tight_layout()\n    plt.savefig('confusion_matrix.png', bbox_inches=\"tight\", transparent=True)\n    plt.show()\n    \ndef plot_clf_report(classes, plotMat, support, cmap=plt.cm.Blues):\n    title = 'Classification Report'\n    xticklabels = ['Precision', 'Recall', 'F1-score']\n    yticklabels = ['{0} ({1})'.format(classes[idx], sup) for idx, sup in enumerate(support)]\n    plt.clf()\n    fig, ax = plt.subplots(figsize=(20,10))\n    ax.set_title(title, y=-0.06, fontsize=22)\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.set_label_position('top')\n    ax.xaxis.set_tick_params(labelsize=18)\n    ax.yaxis.set_tick_params(labelsize=14)\n    plt.imshow(plotMat, interpolation='nearest', cmap=cmap, aspect='auto')\n    plt.clim(vmin=0.0, vmax=1.0)\n    plt.colorbar()\n    plt.xticks(np.arange(3), xticklabels, rotation=0)\n    plt.yticks(np.arange(len(classes)), yticklabels)\n    thresh = np.max(plotMat) / 2\n    thresh = 1 / 2\n    for i in range(plotMat.shape[0]):\n        for j in range(plotMat.shape[1]):\n            color = \"white\" if (plotMat[i, j] > thresh) else \"black\"\n            plt.text(j, i, format(plotMat[i, j], '.2f'), horizontalalignment=\"center\", color=color, fontsize=14)\n\n    plt.xlabel('Metrics',fontsize=22)\n    plt.ylabel('Classes',fontsize=22)\n    plt.tight_layout()\n    plt.savefig('classification_report.png', bbox_inches=\"tight\", transparent=True)\n    plt.show()\n    \ndef plot_tag_scores(classes, scores, normalize=True):\n    plt.clf()\n    width = 0.45\n    fig, ax = plt.subplots(figsize=(20,10))\n    ax.xaxis.set_tick_params(labelsize=18, rotation=25)\n    ax.yaxis.set_tick_params(labelsize=18)\n    range_bar1 = np.arange(len(classes))\n    rects1 = ax.bar(range_bar1, tuple(scores[:, 0]), width, color='b')\n    rects2 = ax.bar(range_bar1 + width, tuple(scores[:, 1]), width, color='r')\n\n    ax.set_ylabel('Scores',fontsize=22)\n    ax.set_title('Tag scores', fontsize=22)\n    ax.set_xticks(range_bar1 + width / 2)\n    ax.set_xticklabels(classes)\n\n    ax.legend((rects1[0], rects2[0]), ('Correct', 'Wrong'), fontsize=20)\n    plt.legend()\n    plt.savefig('tag_scores.png', bbox_inches=\"tight\", transparent=True)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true=test_labels\ny_pred=final_pred\nprint(len(y_true))\nprint(len(final_pred))\nclasses = ['B','I','O']\nclass_names, report, support = get_report(y_true, y_pred, classes)\ncm = confusion_matrix(y_true, y_pred, labels=classes)\nscores = get_scores(y_true, y_pred, classes)\nplot_clf_report(class_names, report, support)\nplot_confusion_matrix(classes, cm)\nplot_tag_scores(classes, scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"errswords=[]\ncorrectlabels=[]\nerrlabels=[]\n# ff=open('../input/chunking-preds/preds_pos.pkl','rb')\n# allpreds=pickle.load(ff)\nj=0\nfor i in range(len(test_labels)):\n    if(i>indl_list[j]):\n        j+=1\n    if test_labels[i]!=final_pred[i]:\n        errswords.append([allWords[i-1] if ((i>0 and j==0) or (j>0 and i>indl_list[j-1]+1)) else '##',allWords[i],allWords[i+1] if i<indl_list[j]-1 else '##'])\n        correctlabels.append([test_labels[i-1] if ((i>0 and j==0) or (j>0 and i>indl_list[j-1]+1)) else '##',test_labels[i],test_labels[i+1] if i<indl_list[j]-1 else '##'])\n        errlabels.append([final_pred[i-1] if ((i>0 and j==0) or (j>0 and i>indl_list[j-1]+1)) else '##',final_pred[i],final_pred[i+1] if i<indl_list[j]-1 else '##'])\n# for x,y,z in zip(allWords,test_labels,final_pred):\n#     for i in range(len(y)):\n#         if y[i]!=z[i]:\n# #             print('lol')\n#             errswords.append([x[i-1] if i>0 else '##',x[i],x[i+1] if i<len(x)-1 else '##'])\n#             correctlabels.append([y[i-1] if i>0 else '##',y[i],y[i+1] if i<len(y)-1 else '##'])\n#             errlabels.append([z[i-1] if i>0 else '##',z[i],z[i+1] if i<len(z)-1 else '##'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(errswords[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(errswords))\n[print((str(i[0])  if i[0]!=None else '##' )+' '+str(i[1])+' '+(str(i[2])  if i[2]!=None else '##' )+' ( correct: '+j[0]+' '+j[1]+' '+j[2]+ ' predicted: '+k[0]+' '+k[1]+' '+k[2]+' )') for i,j,k in zip(errswords[:100],correctlabels[:100],errlabels[:100])]","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}