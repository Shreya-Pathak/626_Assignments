{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":1,"outputs":[{"output_type":"stream","text":"Running on TPU  grpc://10.0.0.2:8470\nREPLICAS:  8\n","name":"stdout"}]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"from tensorflow import keras\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom gensim.models import KeyedVectors","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"DATA_DIR = '../input/conll-corpora/conll2000/conll2000/{}.txt'\n\ndef get_data(file):\n    with open(file, 'r', encoding='latin1') as fp:\n        content = fp.readlines()\n    data, sent = [], []\n    for line in content:\n        if not line.strip():\n            if sent: data.append(sent)\n            sent = []\n        else:\n            word, pos, tag = line.strip().split()\n            tag = tag.split('-')[0]\n            sent.append((word, pos, tag))\n    return data","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = get_data(DATA_DIR.format('train'))\ntest_data = get_data(DATA_DIR.format('test'))","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MAX_LEN = 78\nempty_token = '<UNK>'\nempty_pos = '^'\nempty_tag = '$'\npad_value = 0\nembed_dim = 300","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sentences_train = [' '.join([tup[0].lower() for tup in sent]) for sent in train_data]\nsent_tokenizer = Tokenizer(oov_token=empty_token, filters='\\t\\n') \nsent_tokenizer.fit_on_texts(sentences_train)\nsentences_train = sent_tokenizer.texts_to_sequences(sentences_train)\nsentences_train = pad_sequences(sentences_train, padding='post', value=pad_value, maxlen=MAX_LEN)\nNUM_WORDS = len(sent_tokenizer.word_index)\n\nsentences_test = [' '.join([tup[0].lower() for tup in sent]) for sent in test_data]\nsentences_test = sent_tokenizer.texts_to_sequences(sentences_test)\nsentences_test = pad_sequences(sentences_test, padding='post', value=pad_value, maxlen=MAX_LEN)","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"postags_train = [' '.join([tup[1].lower() for tup in sent]) for sent in train_data]\npos_tokenizer = Tokenizer(oov_token=empty_pos, filters='\\t\\n') \npos_tokenizer.fit_on_texts(postags_train)\npostags_train = pos_tokenizer.texts_to_sequences(postags_train)\npostags_train = pad_sequences(postags_train, padding='post', value=pad_value, maxlen=MAX_LEN)\nNUM_POS = len(pos_tokenizer.word_index)\n\npostags_test = [' '.join([tup[1].lower() for tup in sent]) for sent in test_data]\npostags_test = pos_tokenizer.texts_to_sequences(postags_test)\npostags_test = pad_sequences(postags_test, padding='post', value=pad_value, maxlen=MAX_LEN)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tags_train = [[tup[2] for tup in sent] for sent in train_data]\ntag_tokenizer = Tokenizer(oov_token=empty_tag, filters='\\t\\n')\ntag_tokenizer.fit_on_texts(tags_train)\ntags_train = tag_tokenizer.texts_to_sequences(tags_train)\ntags_train = pad_sequences(tags_train, padding='post', value=pad_value, maxlen=MAX_LEN)\nNUM_TAGS = len(tag_tokenizer.word_index)\n\ntags_test = [[tup[2] for tup in sent] for sent in test_data]\ntags_test = tag_tokenizer.texts_to_sequences(tags_test)\ntags_test = pad_sequences(tags_test, padding='post', value=pad_value, maxlen=MAX_LEN)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def load_glove():\n    file = f'../input/nlpword2vecembeddingspretrained/glove.6B.{embed_dim}d.txt'\n    embeddings_index = dict()\n    f = open(file)\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word] = coefs\n    f.close()\n    print('Loaded %s word vectors.' % len(embeddings_index))\n\n    all_embs = np.stack(list(embeddings_index.values()))\n    emb_mean, emb_std = np.mean(all_embs), np.std(all_embs)\n    embedding_matrix = np.random.normal(emb_mean, emb_std, (NUM_WORDS + 1, embed_dim))\n    embeddedCount = 0\n    not_found = []\n    for word, idx in sent_tokenizer.word_index.items():\n        embedding_vector = embeddings_index.get(word.lower())\n        if embedding_vector is not None: \n            embedding_matrix[idx] = embedding_vector\n            embeddedCount += 1\n    print('total embedded:',embeddedCount,'common words')\n\n    del(embeddings_index)\n    del(all_embs)\n    return embedding_matrix\n\ndef load_word2vec():\n    file = '../input/nlpword2vecembeddingspretrained/GoogleNews-vectors-negative300.bin'\n    word2vec = KeyedVectors.load_word2vec_format(file, binary=True)\n    embedding_matrix = np.random.normal(size=(NUM_WORDS + 1, embed_dim))\n    for word, idx in sent_tokenizer.word_index.items():\n        if word in word2vec.vocab:\n            embedding_matrix[idx] = word2vec.word_vec(word)\n    return embedding_matrix","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"glove = load_glove()","execution_count":10,"outputs":[{"output_type":"stream","text":"Loaded 400000 word vectors.\ntotal embedded: 15794 common words\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# wvec = load_word2vec()","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def ignore_accuracy_of_class(class_to_ignore=0):\n    def acc(y_true, y_pred):\n        y_true_class=tf.cast(y_true, tf.int64)\n        y_pred_class = K.argmax(y_pred, axis=-1)\n        ignore_mask = K.cast(K.not_equal(y_pred_class, class_to_ignore), 'int32')\n        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n        return accuracy\n    return acc\n\ncustom_acc = ignore_accuracy_of_class(pad_value)","execution_count":12,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, LSTM, Input, Bidirectional, TimeDistributed, Embedding, Concatenate\n\ndef model_without_pos():\n    with strategy.scope():\n        custom_emb = keras.initializers.Constant(glove)\n        regularizer = tf.keras.regularizers.l1_l2(l1=1e-4, l2=1e-3)\n        inputs = Input(shape=(MAX_LEN,), dtype='int32')\n        word_emb = Embedding(NUM_WORDS + 1, embed_dim, embeddings_initializer=custom_emb, trainable=True)(inputs)\n        lstm = Bidirectional(LSTM(32, return_sequences=True, kernel_regularizer=regularizer))(word_emb)\n        td = TimeDistributed(Dense(NUM_TAGS + 1, activation='softmax', kernel_regularizer=regularizer))(lstm)\n        model = Model(inputs=[inputs], outputs=[td])\n        return model\n    \ndef model_with_pos():\n    with strategy.scope():\n        custom_emb = keras.initializers.Constant(glove)\n        regularizer = tf.keras.regularizers.l1_l2(l1=1e-4, l2=1e-3)\n        word_inputs = Input(shape=(MAX_LEN,), dtype='int32')\n        pos_inputs = Input(shape=(MAX_LEN,), dtype='int32')\n        word_emb = Embedding(NUM_WORDS + 1, embed_dim, embeddings_initializer=custom_emb, trainable=True)(word_inputs)\n        pos_emb = Embedding(NUM_POS + 1, 25, trainable=True)(pos_inputs)\n        emb = Concatenate(axis=-1)([word_emb, pos_emb])\n        lstm = Bidirectional(LSTM(32, return_sequences=True, kernel_regularizer=regularizer))(emb)\n        td = TimeDistributed(Dense(NUM_TAGS + 1, activation='softmax', kernel_regularizer=regularizer))(lstm)\n        model = Model(inputs=[word_inputs, pos_inputs], outputs=[td])\n        return model","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"K.clear_session()\nmodel = model_with_pos()\nprint(model.summary())","execution_count":18,"outputs":[{"output_type":"stream","text":"Model: \"model\"\n__________________________________________________________________________________________________\nLayer (type)                    Output Shape         Param #     Connected to                     \n==================================================================================================\ninput_1 (InputLayer)            [(None, 78)]         0                                            \n__________________________________________________________________________________________________\ninput_2 (InputLayer)            [(None, 78)]         0                                            \n__________________________________________________________________________________________________\nembedding (Embedding)           (None, 78, 300)      5178000     input_1[0][0]                    \n__________________________________________________________________________________________________\nembedding_1 (Embedding)         (None, 78, 25)       1150        input_2[0][0]                    \n__________________________________________________________________________________________________\nconcatenate (Concatenate)       (None, 78, 325)      0           embedding[0][0]                  \n                                                                 embedding_1[0][0]                \n__________________________________________________________________________________________________\nbidirectional (Bidirectional)   (None, 78, 64)       91648       concatenate[0][0]                \n__________________________________________________________________________________________________\ntime_distributed (TimeDistribut (None, 78, 5)        325         bidirectional[0][0]              \n==================================================================================================\nTotal params: 5,271,123\nTrainable params: 5,271,123\nNon-trainable params: 0\n__________________________________________________________________________________________________\nNone\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n\nlosses, val_losses = [], []\naccs, val_accs = [], []\nmodel_name = 'bilstm_chunker.h5'\nstopper = EarlyStopping(monitor='acc', patience=5, mode='max')\ncheckpointer = ModelCheckpoint(filepath=model_name, monitor='val_acc', mode='max', save_best_only=True, verbose=2)\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(0.001), metrics=[custom_acc])","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"history = model.fit(\n    x = [sentences_train, postags_train],\n    y = tags_train,\n    validation_data = ([sentences_test, postags_test], tags_test),\n    callbacks = [stopper],\n    epochs = 80,\n    batch_size = 128 * strategy.num_replicas_in_sync,\n    verbose = 1,\n)\n\nlosses += list(history.history['loss'])\nval_losses += list(history.history['val_loss'])\n\naccs += list(history.history['acc'])\nval_accs += list(history.history['val_acc'])","execution_count":null,"outputs":[{"output_type":"stream","text":"Epoch 1/60\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1, 2, figsize=(18, 6))\nax[0].plot(losses, label='Loss (training data)')\nax[0].plot(val_losses, label='Loss (validation data)')\nax[0].set_title('Loss Trend')\nax[0].set_ylabel('Loss value')\nax[0].set_xlabel('No. of epochs')\nax[0].legend(loc=\"upper left\")\n\nax[1].plot(accs, label='Accuracy (training data)')\nax[1].plot(val_accs, label='Accuracy (validation data)')\nax[1].set_title('Accuracy Trend')\nax[1].set_ylabel('Accuracy value')\nax[1].set_xlabel('No. of epochs')\nax[1].legend(loc=\"upper left\")\n\nplt.tight_layout()\n\nplt.savefig('loss_acc_trend.png', bbox_inches='tight')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def pred_labels(sentences_test, tags_test, given_model):\n    tags_pred = np.argmax(given_model.predict(sentences_test), axis=-1)\n    y_true, y_pred = [], []\n    inv_map = {v:k for k,v in tag_tokenizer.word_index.items()}\n    for i in range(len(tags_test)):\n        for j in range(len(tags_test[i])):\n            # we have reached padding\n            if tags_test[i][j] == pad_value : \n                break\n            # map padding class to $ class\n            if tags_pred[i][j] == pad_value :\n                tags_pred[i][j] = tag_tokenizer.word_index[empty_tag]\n            y_pred.append(inv_map[tags_pred[i][j]].upper())\n            y_true.append(inv_map[tags_test[i][j]].upper())\n    return np.array(y_true), np.array(y_pred)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"y_true, y_pred = pred_labels([sentences_test, postags_test], tags_test, model)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\ndef get_report(y_true, y_pred, classes):\n    clf_report = classification_report(y_true, y_pred, labels=classes, zero_division=0)\n    clf_report = clf_report.replace('\\n\\n', '\\n')\n    clf_report = clf_report.replace('micro avg', 'micro_avg')\n    clf_report = clf_report.replace('macro avg', 'macro_avg')\n    clf_report = clf_report.replace('weighted avg', 'weighted_avg')\n    clf_report = clf_report.replace(' / ', '/')\n    lines = clf_report.split('\\n')\n\n    class_names, plotMat, support = [], [], []\n    for line in lines[1:]:\n        t = line.strip().split()\n        if len(t) < 2:\n            continue\n        v = [float(x) for x in t[1: len(t) - 1]]\n        if len(v) == 1 : v = v * 3\n        support.append(int(t[-1]))\n        class_names.append(t[0])\n        plotMat.append(v)\n    plotMat = np.array(plotMat)\n    support = np.array(support)\n    return class_names, plotMat, support\n\ndef get_scores(y_true, y_pred, classes):\n    correct, wrong = {}, {}\n    for tag in classes:\n        correct[tag] = 0\n        wrong[tag] = 0\n        \n    for tag, pred in zip(y_true, y_pred):\n        if tag == pred:\n            correct[tag] += 1\n        else:\n            wrong[tag] += 1\n            \n    scores = []\n    total = len(y_true)\n    for tag in classes:\n        cur = np.array([correct[tag], wrong[tag]])\n        scores.append(cur / total)\n    return np.array(scores)\n    \ndef plot_confusion_matrix(classes, mat, normalize=True, cmap=plt.cm.Blues):\n    cm = np.copy(mat)\n    title = 'Confusion Matrix (without normalization)'\n    if normalize:\n        cm = cm.astype('float') / np.sum(cm, axis=1, keepdims=True)\n        title = title.replace('without', 'with')\n    plt.clf()    \n    fig, ax = plt.subplots(figsize=(20,10))\n    ax.set_title(title, y=-0.06, fontsize=22)\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.set_label_position('top')\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.clim(vmin=0.0, vmax=1.0)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = '.2f' if normalize else 'd'\n    thresh = np.max(cm) / 2\n    thresh = 1 / 2\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            color = \"white\" if (cm[i, j] > thresh) else \"black\"\n            plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=color)\n    plt.ylabel('True label',fontsize=22)\n    plt.xlabel('Predicted label', fontsize=22)\n    plt.tight_layout()\n    plt.savefig('confusion_matrix.png', bbox_inches=\"tight\", transparent=True)\n    \ndef plot_clf_report(classes, plotMat, support, cmap=plt.cm.Blues):\n    title = 'Classification Report'\n    xticklabels = ['Precision', 'Recall', 'F1-score']\n    yticklabels = ['{0} ({1})'.format(classes[idx], sup) for idx, sup in enumerate(support)]\n    plt.clf()\n    fig, ax = plt.subplots(figsize=(20,10))\n    ax.set_title(title, y=-0.06, fontsize=22)\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.set_label_position('top')\n    ax.xaxis.set_tick_params(labelsize=18)\n    ax.yaxis.set_tick_params(labelsize=14)\n    plt.imshow(plotMat, interpolation='nearest', cmap=cmap, aspect='auto')\n    plt.clim(vmin=0.0, vmax=1.0)\n    plt.colorbar()\n    plt.xticks(np.arange(3), xticklabels, rotation=0)\n    plt.yticks(np.arange(len(classes)), yticklabels)\n    thresh = np.max(plotMat) / 2\n    thresh = 1 / 2\n    for i in range(plotMat.shape[0]):\n        for j in range(plotMat.shape[1]):\n            color = \"white\" if (plotMat[i, j] > thresh) else \"black\"\n            plt.text(j, i, format(plotMat[i, j], '.2f'), horizontalalignment=\"center\", color=color, fontsize=14)\n\n    plt.xlabel('Metrics',fontsize=22)\n    plt.ylabel('Classes',fontsize=22)\n    plt.tight_layout()\n    plt.savefig('classification_report.png', bbox_inches=\"tight\", transparent=True)\n    \ndef plot_tag_scores(classes, scores, normalize=True):\n    plt.clf()\n    width = 0.45\n    fig, ax = plt.subplots(figsize=(20,10))\n    ax.xaxis.set_tick_params(labelsize=18, rotation=25)\n    ax.yaxis.set_tick_params(labelsize=18)\n    range_bar1 = np.arange(len(classes))\n    rects1 = ax.bar(range_bar1, tuple(scores[:, 0]), width, color='b')\n    rects2 = ax.bar(range_bar1 + width, tuple(scores[:, 1]), width, color='r')\n\n    ax.set_ylabel('Scores',fontsize=22)\n    ax.set_title('Tag scores', fontsize=22)\n    ax.set_xticks(range_bar1 + width / 2)\n    ax.set_xticklabels(classes)\n\n    ax.legend((rects1[0], rects2[0]), ('Correct', 'Wrong'), fontsize=20)\n    plt.legend()\n    plt.savefig('tag_scores.png', bbox_inches=\"tight\", transparent=True)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = sorted([c.upper() for c in tag_tokenizer.word_index.keys() if c != empty_tag])\nclass_names, report, support = get_report(y_true, y_pred, classes)\ncm = confusion_matrix(y_true, y_pred, labels=classes)\nscores = get_scores(y_true, y_pred, classes)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_clf_report(class_names, report, support)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(classes, cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_tag_scores(classes, scores)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}