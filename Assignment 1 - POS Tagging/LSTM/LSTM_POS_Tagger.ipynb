{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import nltk\nimport tensorflow as tf\nfrom keras import backend as K\nimport keras\nimport string\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\nfrom keras.optimizers import Adam\nfrom sklearn.model_selection import KFold\nnltk.download('brown')\nnltk.download('universal_tagset')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tag_sent = nltk.corpus.brown.tagged_sents(tagset='universal')\nprint(len(tag_sent))\n# po=set([y[1] for x in tag_sent for y in x])\nsentences=[[x[0] for x in y] for y in tag_sent]\n#print(len(sentences))\ntags=[[x[1] for x in y] for y in tag_sent]\n#print(sentences[0])\n\n# tmp1,tmp2=[],[]\n# for x,x1 in zip(sentences,tags):\n#     new_sent=[]\n#     new_punc=[]\n#     for y,y1 in zip(x,x1):\n#     # if y in '!\"#$%&()*+,-/:;<=>?@[\\\\]^_\\'`{|}~\\t\\n':\n#         if len(y.translate({ord(x):None for x in '!.\"#$%&()*+,-/:;<=>?@[\\\\]^_\\'`{|}~\\t\\n'}))==0:\n#             pass\n#         else:\n#             new_sent.append(y)\n#             new_punc.append(y1)\n#             tmp1.append(new_sent)\n#             tmp2.append(new_punc)\n# sentences=tmp1\n# tags=tmp2\n\ndef concat(li):\n    sent=''\n    for x in li:\n        if len(sent)!=0:\n            sent=sent+' '\n        sent=sent+x\n    return sent\nMAX_LEN=60\nsentences=[concat(x).lower() for x in sentences]\n#print(sentences[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_WORDS=50000\ntok=keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS,oov_token='<OOV>',filters='\\t\\n')\ntok.fit_on_texts(sentences)\n# print(tok.get_config())\n# print(tok.word_index)\nsentences=tok.texts_to_sequences(sentences)\n\nsentences=keras.preprocessing.sequence.pad_sequences(sentences,padding='post',maxlen=MAX_LEN)\n# print(a[4])\n\n# print(len(set([x for y in tags for x in y]+['$'])))\n\n#print('koko')\ntags=keras.preprocessing.sequence.pad_sequences(tags,padding='post',value='$',maxlen=MAX_LEN,dtype=object).tolist()\n#print(tags[0])\n# NUM_TAGS=set([x for y in tags for x in y]+['$'])\n#print(len(tags[0]))\ntok_tag=keras.preprocessing.text.Tokenizer(oov_token='<UNK>',filters='\\t\\n')\n\n\ntok_tag.fit_on_texts(tags)\nnum_tags=len(tok_tag.word_index)\ntags=tok_tag.texts_to_sequences(tags)\ntags=np.array(tags)\nprint('done')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(tok_tag.word_index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(tok_tag.word_index['$'])\ndef ignore_accuracy_of_class(class_to_ignore=0):\n    def acc(y_true, y_pred):\n        y_true_class=tf.cast(y_true,tf.int64)\n#         y_true_class = K.argmax(y_true, axis=-1)\n        y_pred_class = K.argmax(y_pred, axis=-1)\n#         print((y_true_class).dtype)\n#         print((y_pred_class).dtype)\n        ignore_mask = K.cast(K.not_equal(y_pred_class, class_to_ignore), 'int32')\n        matches = K.cast(K.equal(y_true_class, y_pred_class), 'int32') * ignore_mask\n        accuracy = K.sum(matches) / K.maximum(K.sum(ignore_mask), 1)\n        return accuracy\n\n    return acc\ncustacc=ignore_accuracy_of_class(tok_tag.word_index['$'])","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"def remove_eos(actual,pred):\n    ansact,anspred=[],[]\n    for x,y in zip(actual,pred):\n        if x=='$':\n            return (ansact,anspred)\n        else :\n            ansact.append(x)\n            anspred.append(y)\n    return (ansact,anspred)\n\ndef get_lists(actual,pred): #give the input as list of list\n    global tok_tag\n    rev = dict(map(reversed, tok_tag.word_index.items()))\n    actual=[[rev[x] for x in y] for y in actual]\n    pred=[[rev[x] for x in y] for y in pred]\n    cleaned=[remove_eos(x,y) for x,y in zip(actual,pred)]\n    a1,a2=[],[]\n    for x in cleaned:\n        for a in x[0]:\n            a1.append(a)\n        for a in x[1]:\n            a2.append(a)\n    return a1,a2","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"kfold = KFold(5, True, 1)\ny_trues,y_preds=[],[]\nmiswords,mistags,mispreds=[],[],[]\nfor train,test in kfold.split(sentences):\n    with strategy.scope():\n        model = Sequential()\n        model.add(InputLayer(input_shape=(MAX_LEN,), dtype='int32'))\n        # model.add(InputLayer(input_shape=(MAX_LEN,)))\n        model.add(Embedding(NUM_WORDS+1, 64,trainable=True))\n        model.add(Bidirectional(LSTM(64, return_sequences=True,kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-4, l2=1e-3))))\n        model.add(TimeDistributed(Dense(num_tags+1,activation='softmax',kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-4, l2=1e-3))))\n    model.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(0.001),metrics=[custacc])\n#     print(len(train))\n    xtrain=sentences[train]\n#     print(np.shape(xtrain))\n    ytrain=tags[train]\n    xtest=sentences[test]\n    ytest=tags[test]\n    model.fit(x=xtrain,y=ytrain,validation_data=(xtest,ytest),epochs=37,batch_size = 128 * strategy.num_replicas_in_sync)\n    ypred=model.predict(xtest)\n#     print(xtest.shape)\n#     print(ypred.shape)\n    ypred=np.argmax(ypred,axis=2)\n    for i in range(min(np.shape(xtest)[0],np.shape(ypred)[0])):\n#         print(np.shape(ypred[i,:]))\n#         print(np.shape(ypred))\n        for j in range(np.shape(ypred[i,:])[0]):\n            if ypred[i][j]!=ytest[i][j] and ytest[i][j]!=tok_tag.word_index['$']:\n                miswords.append(xtest[i,max(0,j-1):j+2])\n                mistags.append(ytest[i,max(0,j-1):j+2])\n                mispreds.append(ypred[i,max(0,j-1):j+2])\n    #misclassis.append(misclassi)\n#     print(ypred.shape)\n#     print(ytest.shape)\n    ypred=ypred.tolist()\n    ytest=ytest.tolist()\n    a1,a2=get_lists(ytest,ypred)\n    y_trues.append(a1)\n    y_preds.append(a2)\n    ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#print(len(misclassis[0]))\nidxword={v:k for k,v in tok.word_index.items()}\nidxtag={v:k for k,v in tok_tag.word_index.items()}\nwordtri=[[idxword.get(z,'<UNK>') for z in x]for x in miswords]\ntagtri=[[idxtag.get(z,'<UNK>') for z in x]for x in mistags]\npredtri=[[idxtag.get(z,'<UNK>') for z in x]for x in mispreds]\n# print(tagtri[30:40])\n# print(predtri[30:40])\n# print(wordtri[30:40])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"di={}\nfor x in tagtri:\n    di[tuple(x)]=di.get(tuple(x),0)+1\ninv=[(k,v) for v,k in di.items()]\ninv.sort(reverse=True)\nprint(inv)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom matplotlib import pyplot as plt\n\ndef get_report(y_true, y_pred, classes):\n    clf_report = classification_report(y_true, y_pred, labels=classes, zero_division=0)\n    clf_report = clf_report.replace('\\n\\n', '\\n')\n    clf_report = clf_report.replace('macro avg', 'macro_avg')\n    clf_report = clf_report.replace('micro avg', 'micro_avg')\n    clf_report = clf_report.replace('weighted avg', 'weighted_avg')\n    clf_report = clf_report.replace(' / ', '/')\n    lines = clf_report.split('\\n')\n\n    class_names, plotMat, support = [], [], []\n    for line in lines[1:]:\n        t = line.strip().split()\n        if len(t) < 2:\n            continue\n        v = [float(x) for x in t[1: len(t) - 1]]\n        if len(v) == 1 : v = v * 3\n        support.append(int(t[-1]))\n        class_names.append(t[0])\n        plotMat.append(v)\n    plotMat = np.array(plotMat)\n    support = np.array(support)\n    return class_names, plotMat, support\n\ndef get_scores(y_true, y_pred, classes):\n    correct, wrong = {}, {}\n    for tag in classes:\n        correct[tag] = 0\n        wrong[tag] = 0\n        \n    for tag, pred in zip(y_true, y_pred):\n        if tag in correct and tag == pred:\n            correct[tag] += 1\n        elif tag in wrong:\n            wrong[tag] += 1\n            \n    scores = []\n    total = len(y_true)\n    for tag in classes:\n        cur = np.array([correct[tag], wrong[tag]])\n        scores.append(cur / total)\n    return np.array(scores)\n    \ndef plot_confusion_matrix(classes, mat, normalize=True, cmap=plt.cm.Blues):\n    cm = np.copy(mat)\n    title = 'Confusion Matrix (without normalization)'\n    if normalize:\n        cm = cm.astype('float') / np.sum(cm, axis=1, keepdims=True)\n        title = title.replace('without', 'with')\n    plt.clf()    \n    fig, ax = plt.subplots(figsize=(20,10))\n    ax.set_title(title, y=-0.06, fontsize=22)\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.set_label_position('top')\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    fmt = '.2f' if normalize else 'd'\n    thresh = np.max(cm) / 2\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            color = \"white\" if (cm[i, j] > thresh) else \"black\"\n            plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=color)\n    plt.ylabel('True label',fontsize=22)\n    plt.xlabel('Predicted label', fontsize=22)\n    plt.tight_layout()\n    plt.savefig('confusion_matrix.png', bbox_inches=\"tight\", transparent=True)\n    \ndef plot_clf_report(classes, plotMat, support, cmap=plt.cm.Blues):\n    title = 'Classification Report'\n    xticklabels = ['Precision', 'Recall', 'F1-score']\n    yticklabels = ['{0} ({1})'.format(classes[idx], sup) for idx, sup in enumerate(support)]\n    plt.clf()\n    fig, ax = plt.subplots(figsize=(20,10))\n    ax.set_title(title, y=-0.06, fontsize=22)\n    ax.xaxis.set_ticks_position('top')\n    ax.xaxis.set_label_position('top')\n    ax.xaxis.set_tick_params(labelsize=18)\n    ax.yaxis.set_tick_params(labelsize=14)\n    plt.imshow(plotMat, interpolation='nearest', cmap=cmap, aspect='auto')\n    plt.colorbar()\n    plt.xticks(np.arange(3), xticklabels, rotation=0)\n    plt.yticks(np.arange(len(classes)), yticklabels)\n\n    thresh = np.max(plotMat) / 2\n    for i in range(plotMat.shape[0]):\n        for j in range(plotMat.shape[1]):\n            color = \"white\" if (plotMat[i, j] > thresh) else \"black\"\n            plt.text(j, i, format(plotMat[i, j], '.2f'), horizontalalignment=\"center\", color=color, fontsize=14)\n\n    plt.xlabel('Metrics',fontsize=22)\n    plt.ylabel('Classes',fontsize=22)\n    plt.tight_layout()\n    plt.savefig('classification_report.png', bbox_inches=\"tight\", transparent=True)\n    \ndef plot_tag_scores(classes, scores, normalize=True):\n    plt.clf()\n    width = 0.45\n    fig, ax = plt.subplots(figsize=(20,10))\n    ax.xaxis.set_tick_params(labelsize=18, rotation=25)\n    ax.yaxis.set_tick_params(labelsize=18)\n    range_bar1 = np.arange(len(classes))\n    rects1 = ax.bar(range_bar1, tuple(scores[:, 0]), width, color='b')\n    rects2 = ax.bar(range_bar1 + width, tuple(scores[:, 1]), width, color='r')\n\n    ax.set_ylabel('Scores',fontsize=22)\n    ax.set_title('Tag scores', fontsize=22)\n    ax.set_xticks(range_bar1 + width / 2)\n    ax.set_xticklabels(classes)\n\n    ax.legend((rects1[0], rects2[0]), ('Correct', 'Wrong'), fontsize=20)\n    plt.legend()\n    plt.savefig('tag_scores.png', bbox_inches=\"tight\", transparent=True)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"classes = labels\nclass_names = None\nreport = None\nsupport = None\ncm, scores = None, None\ncnt = 0\nprint(classes)\nfor y_true, y_pred in zip(y_trues, y_preds):\n    class_names, report_, support_ = get_report(y_true, y_pred, classes)\n    cm_ = confusion_matrix(y_true, y_pred, labels=classes)\n    scores_ = get_scores(y_true, y_pred, classes)\n    \n    if report is None : report = np.zeros_like(report_, dtype=np.float64)\n    report += report_\n    \n    if support is None : support = np.zeros_like(support_, dtype=np.float64)\n    support += support_\n    \n    if cm is None : cm = np.zeros_like(cm_, dtype=np.float64)\n    cm += cm_\n    \n    if scores is None : scores = np.zeros_like(scores_, dtype=np.float64)\n    scores += scores_\n    \n    cnt += 1\n    \nreport /= cnt\nsupport /= cnt\ncm /= cnt\nscores /= cnt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_clf_report(class_names, report, support)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_confusion_matrix(classes, cm)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plot_tag_scores(classes, scores)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}