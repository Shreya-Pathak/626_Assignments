{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import nltk\nimport tensorflow as tf\nimport keras\nimport string\nfrom sklearn.preprocessing import LabelEncoder\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, InputLayer, Bidirectional, TimeDistributed, Embedding, Activation\nfrom keras.optimizers import Adam\nnltk.download('brown')","execution_count":1,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package brown to /usr/share/nltk_data...\n[nltk_data]   Package brown is already up-to-date!\n","name":"stdout"},{"output_type":"execute_result","execution_count":1,"data":{"text/plain":"True"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nAUTO = tf.data.experimental.AUTOTUNE\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","execution_count":2,"outputs":[{"output_type":"stream","text":"Running on TPU  grpc://10.0.0.2:8470\nREPLICAS:  8\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"tag_sent = nltk.corpus.brown.tagged_sents()\n\n# po=set([y[1] for x in tag_sent for y in x])\nsentences=[[x[0] for x in y] for y in tag_sent]\n\ntags=[[x[1] for x in y] for y in tag_sent]\nprint(sentences[0])\n\n# tmp1,tmp2=[],[]\n# for x,x1 in zip(sentences,tags):\n#     new_sent=[]\n#     new_punc=[]\n#     for y,y1 in zip(x,x1):\n#     # if y in '!\"#$%&()*+,-/:;<=>?@[\\\\]^_\\'`{|}~\\t\\n':\n#         if len(y.translate({ord(x):None for x in '!.\"#$%&()*+,-/:;<=>?@[\\\\]^_\\'`{|}~\\t\\n'}))==0:\n#             pass\n#         else:\n#             new_sent.append(y)\n#             new_punc.append(y1)\n#             tmp1.append(new_sent)\n#             tmp2.append(new_punc)\n# sentences=tmp1\n# tags=tmp2\n\ndef concat(li):\n    sent=''\n    for x in li:\n        if len(sent)!=0:\n            sent=sent+' '\n        sent=sent+x\n    return sent\nMAX_LEN=60\nsentences=[concat(x).lower() for x in sentences]\nprint(sentences[0])","execution_count":41,"outputs":[{"output_type":"stream","text":"['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.']\nthe fulton county grand jury said friday an investigation of atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"NUM_WORDS=50000\ntok=keras.preprocessing.text.Tokenizer(num_words=NUM_WORDS,oov_token='<OOV>',filters='\\t\\n')\ntok.fit_on_texts(sentences)\n# print(tok.get_config())\n# print(tok.word_index)\nsentences=tok.texts_to_sequences(sentences)\n\nsentences=keras.preprocessing.sequence.pad_sequences(sentences,padding='post',maxlen=MAX_LEN)\n# print(a[4])\n\n# print(len(set([x for y in tags for x in y]+['$'])))\n\nprint('koko')\ntags=keras.preprocessing.sequence.pad_sequences(tags,padding='post',value='$',maxlen=MAX_LEN,dtype=object).tolist()\nprint(tags[0])\n# NUM_TAGS=set([x for y in tags for x in y]+['$'])\nprint(len(tags[0]))\ntok_tag=keras.preprocessing.text.Tokenizer(oov_token='<UNK>',filters='\\t\\n')\n\n\ntok_tag.fit_on_texts(tags)\nnum_tags=len(tok_tag.word_index)\ntags=tok_tag.texts_to_sequences(tags)\ntags=np.array(tags)\nprint(tags[0])\nprint(len(tags[0]))","execution_count":42,"outputs":[{"output_type":"stream","text":"koko\n['AT', 'NP-TL', 'NN-TL', 'JJ-TL', 'NN-TL', 'VBD', 'NR', 'AT', 'NN', 'IN', 'NP$', 'JJ', 'NN', 'NN', 'VBD', '``', 'AT', 'NN', \"''\", 'CS', 'DTI', 'NNS', 'VBD', 'NN', '.', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$', '$']\n60\n[ 5 42 23 41 23 15 60  5  3  4 50  6  3  3 15 30  5  3 31 16 48  9 15  3\n  7  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2  2\n  2  2  2  2  2  2  2  2  2  2  2  2]\n60\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(tok_tag.word_index['\\'\\''])","execution_count":47,"outputs":[{"output_type":"stream","text":"31\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(len(tags))","execution_count":48,"outputs":[{"output_type":"stream","text":"57340\n","name":"stdout"}]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"with strategy.scope():\n    model = Sequential()\n    model.add(InputLayer(input_shape=(MAX_LEN,), dtype='int32'))\n    # model.add(InputLayer(input_shape=(MAX_LEN,)))\n    model.add(Embedding(NUM_WORDS+1, 128,trainable=True))\n    model.add(Bidirectional(LSTM(128, return_sequences=True,kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-4, l2=1e-3))))\n    model.add(TimeDistributed(Dense(num_tags+1,activation='softmax',kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-4, l2=1e-3))))\n\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer=Adam(0.001),metrics=['accuracy'])\n\nmodel.summary()\nprint(type(sentences))\nprint(type(sentences[0]))\nprint(type(tags[0]))\nprint(type(tags))\n","execution_count":49,"outputs":[{"output_type":"stream","text":"Model: \"sequential_6\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_6 (Embedding)      (None, 60, 128)           6400128   \n_________________________________________________________________\nbidirectional_5 (Bidirection (None, 60, 256)           263168    \n_________________________________________________________________\ntime_distributed_5 (TimeDist (None, 60, 473)           121561    \n=================================================================\nTotal params: 6,784,857\nTrainable params: 6,784,857\nNon-trainable params: 0\n_________________________________________________________________\n<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n<class 'numpy.ndarray'>\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.fit(x=sentences,y=tags,validation_split=0.1,epochs=5,batch_size = 10 * strategy.num_replicas_in_sync)","execution_count":50,"outputs":[{"output_type":"stream","text":"Epoch 1/5\n646/646 [==============================] - 15s 24ms/step - loss: 1.4903 - accuracy: 0.7226 - val_loss: 0.8664 - val_accuracy: 0.8069\nEpoch 2/5\n646/646 [==============================] - 11s 17ms/step - loss: 0.8168 - accuracy: 0.8522 - val_loss: 0.6185 - val_accuracy: 0.9161\nEpoch 3/5\n646/646 [==============================] - 11s 17ms/step - loss: 0.6313 - accuracy: 0.9188 - val_loss: 0.5271 - val_accuracy: 0.9438\nEpoch 4/5\n646/646 [==============================] - 11s 17ms/step - loss: 0.5505 - accuracy: 0.9388 - val_loss: 0.4817 - val_accuracy: 0.9522\nEpoch 5/5\n646/646 [==============================] - 11s 17ms/step - loss: 0.5009 - accuracy: 0.9490 - val_loss: 0.4473 - val_accuracy: 0.9590\n","name":"stdout"},{"output_type":"execute_result","execution_count":50,"data":{"text/plain":"<tensorflow.python.keras.callbacks.History at 0x7fe7b793c350>"},"metadata":{}}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}